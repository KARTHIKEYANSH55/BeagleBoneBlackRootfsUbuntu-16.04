{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_farm_building_blocks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KARTHIKEYANSH55/BeagleBoneBlackRootfsUbuntu-16.04/blob/master/1_farm_building_blocks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPltDefXjSiJ",
        "colab_type": "text"
      },
      "source": [
        "# FARM Building Blocks\n",
        "\n",
        "Welcome to the FARM building blocks tutorial! There are many different ways to make use of this repository, but in this notebook, we will be going through the most import building blocks that will help you harvest the rewards of a successfully trained NLP model.\n",
        "\n",
        "Happy FARMing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2cj67ZBjYSC",
        "colab_type": "text"
      },
      "source": [
        "## 1) Text Classification\n",
        "\n",
        "GermEval 2018 (GermEval2018) (https://projects.fzai.h-da.de/iggsa/) is an open data set containing texts that need to be classified by whether they are offensive or not. There are a set of coarse and fine labels, but here we will only be looking at the coarse set which labels each example as either OFFENSE or OTHER. To tackle this task, we are going to build a classifier that is composed of Google's BERT language model and a feed forward neural network prediction head."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQz1S2d5jfST",
        "colab_type": "text"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeIWk6_4jiQ2",
        "colab_type": "code",
        "outputId": "b54eb39e-e177-450c-f1b9-5894e78b4272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Install FARM\n",
        "!pip install farm==0.3.2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting farm==0.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/cd/e7e42bf91ba506fe1193094cbaa96969730981bb248942756e077ce10832/farm-0.3.2.tar.gz (94kB)\n",
            "\r\u001b[K     |███▌                            | 10kB 32.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 81kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 92kB 3.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from farm==0.3.2) (45.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (from farm==0.3.2) (0.34.2)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from farm==0.3.2) (1.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from farm==0.3.2) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from farm==0.3.2) (1.11.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from farm==0.3.2) (2.21.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from farm==0.3.2) (1.4.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from farm==0.3.2) (0.0)\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Collecting mlflow==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/ec/8c9448968d4662e8354b9c3a62e635f8929ed507a45af3d9fdb84be51270/mlflow-1.0.0-py3-none-any.whl (47.7MB)\n",
            "\u001b[K     |████████████████████████████████| 47.7MB 60kB/s \n",
            "\u001b[?25hCollecting transformers==2.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 56.1MB/s \n",
            "\u001b[?25hCollecting dotmap==1.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/eb/ee5f0358a9e0ede90308d8f34e697e122f191c2702dc4f614eca7770b1eb/dotmap-1.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (from farm==0.3.2) (1.1.1)\n",
            "Collecting flask-restplus\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/a6/b17c848771f96ad039ad9e3ea275e842a16c39c4f3eb9f60ee330b20b6c2/flask_restplus-0.13.0-py2.py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 42.1MB/s \n",
            "\u001b[?25hCollecting flask-cors\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->farm==0.3.2) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->farm==0.3.2) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->farm==0.3.2) (0.9.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->farm==0.3.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->farm==0.3.2) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->farm==0.3.2) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->farm==0.3.2) (2019.11.28)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy>=1.2.0->farm==0.3.2) (1.17.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->farm==0.3.2) (0.22.1)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->farm==0.3.2) (2.2.5)\n",
            "Collecting databricks-cli>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/03/fb0c4d31559d48f15403ad88aefff8ae0dddbbeab273d198c20232a937c2/databricks-cli-0.9.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.3.2) (0.25.3)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.3.2) (3.10.0)\n",
            "Collecting simplejson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/a7b98aa9256c8843f92878966dc3d8d914c14aad97e2c5ce4798d5743e07/simplejson-3.17.0.tar.gz (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 14.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.3.2) (3.13)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.3.2) (1.3.13)\n",
            "Collecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/e9/359dbb77c35c419df0aedeb1d53e71e7e3f438ff64a8fdb048c907404de3/alembic-1.4.1.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 38.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.3.2) (2.6.1)\n",
            "Collecting docker>=3.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/74/379a9d30b1620def158c40b88c43e01c1936a287ebb97afab0699c601c57/docker-4.2.0-py2.py3-none-any.whl (143kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 60.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.3.2) (0.3)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.3.2) (0.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.3.2) (7.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.3.2) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.3.2) (1.2.2)\n",
            "Collecting querystring-parser\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/fa/f54f5662e0eababf0c49e92fd94bf178888562c0e7b677c8941bbbcd1bd6/querystring_parser-1.2.4.tar.gz\n",
            "Requirement already satisfied: gunicorn in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.3.2) (20.0.4)\n",
            "Collecting gitpython>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2f/6a366d56c9b1355b0880be9ea66b166cb3536392638d8d91413ec66305ad/GitPython-3.1.0-py3-none-any.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 55.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 50.9MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 48.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers==2.1.1->farm==0.3.2) (2019.12.20)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask->farm==0.3.2) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask->farm==0.3.2) (1.0.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask->farm==0.3.2) (2.11.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from flask-restplus->farm==0.3.2) (2.6.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from flask-restplus->farm==0.3.2) (2018.9)\n",
            "Collecting aniso8601>=0.82\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/e4/787e104b58eadc1a710738d4e418d7e599e4e778e52cb8e5d5ef6ddd5833/aniso8601-8.0.0-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 339kB/s \n",
            "\u001b[?25hRequirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->farm==0.3.2) (0.15.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->farm==0.3.2) (0.14.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->farm==0.3.2) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->farm==0.3.2) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->farm==0.3.2) (2.8.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from databricks-cli>=0.8.0->mlflow==1.0.0->farm==0.3.2) (0.8.6)\n",
            "Collecting configparser>=0.3.5\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/2a/95ed0501cf5d8709490b1d3a3f9b5cf340da6c433f896bbe9ce08dbe6785/configparser-4.0.2-py2.py3-none-any.whl\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/78/f6ade1e18aebda570eed33b7c534378d9659351cadce2fcbc7b31be5f615/Mako-1.1.2-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 13.6MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Collecting websocket-client>=0.32.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 55.3MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/f5/8f84b3bf9d94bdf2454a302f2fa375832b53660ea532586b8a55ff16ae9a/gitdb-4.0.2-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask->farm==0.3.2) (1.1.1)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/35/d2/27777ab463cd44842c78305fa8097dfba0d94768abbb7e1c4d88f1fa1a0b/smmap-3.0.1-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: farm, seqeval, databricks-cli, simplejson, alembic, querystring-parser, sacremoses\n",
            "  Building wheel for farm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for farm: filename=farm-0.3.2-cp36-none-any.whl size=96130 sha256=6a27e4f9006f97dc8d3c9f989ace051d5bab8aa311e06cbcadfb07f3214711f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/05/72/e466c740107d8f41984a0432fcc14b0d2eda0f9e7e786bc039\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=ca630c16461ab5bb934eb9830c4cfdfedd63ea0d6ce6c1f33e68c13b703e09b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.9.1-cp36-none-any.whl size=83754 sha256=b84d02d65b53479f4c3e07e3f932a923471c4bea5e77b108f20c5eb18705f814\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/27/58/c6ef96e649962e9584a50f58d5b6abafb71a03512b2e381ad1\n",
            "  Building wheel for simplejson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simplejson: filename=simplejson-3.17.0-cp36-cp36m-linux_x86_64.whl size=114209 sha256=85ca13a617427c92f56538f4c45d6a87f5919f72b042e5277009b45e4279d769\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/c0/83/dcd0339abb2640544bb8e0938aab2d069cef55e5647ce6e097\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158154 sha256=b2eee8dc62ccea9be28e9bbe00748385c5204a8db06f36e25af3898688a6dfb0\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/07/f7/12f7370ca47a66030c2edeedcc23dec26ea0ac22dcb4c4a0f3\n",
            "  Building wheel for querystring-parser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for querystring-parser: filename=querystring_parser-1.2.4-cp36-none-any.whl size=7079 sha256=dab26b1cc8e2549bfadb56f0d666cf7d249c1dbe51a0dbb637491b71412cd092\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/41/34/23ebf5d1089a9aed847951e0ee375426eb4ad0a7079d88d41e\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=7bcd3d6baba2689d4864b988c5e910d45ba5745d4b3c08439d8a11be2a10ef8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built farm seqeval databricks-cli simplejson alembic querystring-parser sacremoses\n",
            "Installing collected packages: seqeval, configparser, databricks-cli, simplejson, Mako, python-editor, alembic, websocket-client, docker, querystring-parser, smmap, gitdb, gitpython, mlflow, sacremoses, sentencepiece, transformers, dotmap, aniso8601, flask-restplus, flask-cors, farm\n",
            "Successfully installed Mako-1.1.2 alembic-1.4.1 aniso8601-8.0.0 configparser-4.0.2 databricks-cli-0.9.1 docker-4.2.0 dotmap-1.3.0 farm-0.3.2 flask-cors-3.0.8 flask-restplus-0.13.0 gitdb-4.0.2 gitpython-3.1.0 mlflow-1.0.0 python-editor-1.0.4 querystring-parser-1.2.4 sacremoses-0.0.38 sentencepiece-0.1.85 seqeval-0.0.12 simplejson-3.17.0 smmap-3.0.1 transformers-2.1.1 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6RKA1bxkKij",
        "colab_type": "code",
        "outputId": "60aa42b5-d4b3-47f4-9091-8482436e6f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "source": [
        "# Here are the imports we need\n",
        "\n",
        "import torch\n",
        "from farm.modeling.tokenization import Tokenizer\n",
        "from farm.data_handler.processor import TextClassificationProcessor\n",
        "from farm.data_handler.data_silo import DataSilo\n",
        "from farm.modeling.language_model import LanguageModel\n",
        "from farm.modeling.prediction_head import TextClassificationHead\n",
        "from farm.modeling.adaptive_model import AdaptiveModel\n",
        "from farm.modeling.optimization import initialize_optimizer\n",
        "from farm.train import Trainer\n",
        "from farm.utils import MLFlowLogger\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "03/13/2020 14:57:39 - INFO - transformers.file_utils -   PyTorch version 1.4.0 available.\n",
            "03/13/2020 14:57:40 - INFO - transformers.modeling_xlnet -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
            "03/13/2020 14:57:40 - WARNING - farm.train -   Apex not installed. If you use distributed training with local rank != -1 apex must be installed.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL-on42YOWwC",
        "colab_type": "code",
        "outputId": "3b94f2c3-b52c-47c5-907c-ca06473b5198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# Farm allows simple logging of many parameters & metrics. Let's use the MLflow framework to track our experiment ...\n",
        "# You will see your results on https://public-mlflow.deepset.ai/\n",
        "ml_logger = MLFlowLogger(tracking_uri=\"https://public-mlflow.deepset.ai/\")\n",
        "ml_logger.init_experiment(experiment_name=\"Public_FARM\", run_name=\"Tutorial1_Colab\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " __          __  _                            _        \n",
            " \\ \\        / / | |                          | |       \n",
            "  \\ \\  /\\  / /__| | ___ ___  _ __ ___   ___  | |_ ___  \n",
            "   \\ \\/  \\/ / _ \\ |/ __/ _ \\| '_ ` _ \\ / _ \\ | __/ _ \\ \n",
            "    \\  /\\  /  __/ | (_| (_) | | | | | |  __/ | || (_) |\n",
            "     \\/  \\/ \\___|_|\\___\\___/|_| |_| |_|\\___|  \\__\\___/ \n",
            "  ______      _____  __  __  \n",
            " |  ____/\\   |  __ \\|  \\/  |              _.-^-._    .--.\n",
            " | |__ /  \\  | |__) | \\  / |           .-'   _   '-. |__|\n",
            " |  __/ /\\ \\ |  _  /| |\\/| |          /     |_|     \\|  |\n",
            " | | / ____ \\| | \\ \\| |  | |         /               \\  |\n",
            " |_|/_/    \\_\\_|  \\_\\_|  |_|        /|     _____     |\\ |\n",
            "                                     |    |==|==|    |  |\n",
            "|---||---|---|---|---|---|---|---|---|    |--|--|    |  |\n",
            "|---||---|---|---|---|---|---|---|---|    |==|==|    |  |\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6uTw7w3kPcJ",
        "colab_type": "code",
        "outputId": "e0a019c2-ec2a-48e9-d5fa-b2f704b0d286",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# We need to fetch the right device to drive the growth of our model\n",
        "# Make sure that you have gpu turned on in this notebook by going to\n",
        "# Runtime>Change runtime type and select GPU as Hardware accelerator.\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Devices available: {}\".format(device))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Devices available: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPogG9KLk-Mv",
        "colab_type": "text"
      },
      "source": [
        "### Data Handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-2k0Zock2J5",
        "colab_type": "code",
        "outputId": "6d67333c-336d-4c90-b4a5-f0805c2114c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Here we initialize a tokenizer that will be used for preprocessing text\n",
        "# This is the BERT Tokenizer which uses the byte pair encoding method.\n",
        "# It is currently loaded with a German model\n",
        "\n",
        "tokenizer = Tokenizer.load(\n",
        "    pretrained_model_name_or_path=\"bert-base-cased\",\n",
        "    do_lower_case=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/13/2020 14:57:43 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizer'\n",
            "03/13/2020 14:57:44 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp2kp092x1\n",
            "100%|██████████| 213450/213450 [00:00<00:00, 393957.96B/s]\n",
            "03/13/2020 14:57:45 - INFO - transformers.file_utils -   copying /tmp/tmp2kp092x1 to cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
            "03/13/2020 14:57:45 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
            "03/13/2020 14:57:45 - INFO - transformers.file_utils -   removing temp file /tmp/tmp2kp092x1\n",
            "03/13/2020 14:57:45 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvRVWPveSzK8",
        "colab_type": "code",
        "outputId": "13f3e1e2-ccfb-47a9-beb7-db7b8c3e03cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "tokenizer.tokenize(\"I am about  to ride the train without  a ticket YOLO!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'am',\n",
              " 'about',\n",
              " 'to',\n",
              " 'ride',\n",
              " 'the',\n",
              " 'train',\n",
              " 'without',\n",
              " 'a',\n",
              " 'ticket',\n",
              " 'Y',\n",
              " '##OL',\n",
              " '##O',\n",
              " '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08ycfnEIlJa4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In order to prepare the data for the model, we need a set of\n",
        "# functions to transform data files into PyTorch Datasets.\n",
        "# We group these together in Processor objects.\n",
        "# We will need a new Processor object for each new source of data.\n",
        "# The abstract class can be found in farm.data_handling.processor.Processor\n",
        "\n",
        "processor = TextClassificationProcessor(tokenizer=tokenizer,\n",
        "                                        max_seq_len=128,\n",
        "                                        data_dir=\"../data/germeval18\",\n",
        "                                        label_list = [\"OTHER\", \"OFFENSE\"],\n",
        "                                        metric = \"f1_macro\",\n",
        "                                        label_column_name = \"coarse_label\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zds83m5klLW8",
        "colab_type": "code",
        "outputId": "c19a6cf1-7438-4427-fee0-54637fd23fa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We need a DataSilo in order to keep our train, dev and test sets separate.\n",
        "# The DataSilo will call the functions in the Processor to generate these sets.\n",
        "# From the DataSilo, we can fetch a PyTorch DataLoader object which will\n",
        "# be passed on to the model.\n",
        "# Here is a good place to define a batch size for the model\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "data_silo = DataSilo(\n",
        "    processor=processor,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/10/2019 15:12:19 - INFO - farm.data_handler.data_silo -   \n",
            "Loading data into the data silo ... \n",
            "              ______\n",
            "               |o  |   !\n",
            "   __          |:`_|---'-.\n",
            "  |__|______.-/ _ \\-----.|       \n",
            " (o)(o)------'\\ _ /     ( )      \n",
            " \n",
            "12/10/2019 15:12:19 - INFO - farm.data_handler.data_silo -   Loading train set from: ../data/germeval18/train.tsv \n",
            "12/10/2019 15:12:19 - INFO - farm.data_handler.utils -    Couldn't find ../data/germeval18/train.tsv locally. Trying to download ...\n",
            "12/10/2019 15:12:19 - INFO - farm.data_handler.utils -   downloading and extracting file germeval18 to dir /data\n",
            "100%|██████████| 525101/525101 [00:01<00:00, 399207.47B/s]\n",
            "12/10/2019 15:12:22 - INFO - farm.data_handler.data_silo -   Got ya 2 parallel workers to convert 5009 dictionaries to pytorch datasets (chunksize = 501)...\n",
            "12/10/2019 15:12:22 - INFO - farm.data_handler.data_silo -    0    0 \n",
            "12/10/2019 15:12:22 - INFO - farm.data_handler.data_silo -   /w\\  /w\\\n",
            "12/10/2019 15:12:22 - INFO - farm.data_handler.data_silo -   /'\\  /'\\\n",
            "12/10/2019 15:12:22 - INFO - farm.data_handler.data_silo -     \n",
            "  0%|          | 0/5009 [00:00<?, ? Dicts/s]12/10/2019 15:12:24 - INFO - farm.data_handler.processor -   *** Show 3 random examples ***\n",
            "12/10/2019 15:12:24 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-167-0\n",
            "Clear Text: \n",
            " \ttext: @Frank_Pasemann @Die_Gruenen Wie nennt man Männer die im Krieg ihr Land ihre Landsleute ihre Familien ihre Kameraden im Stich lassen..? |LBR| Deserteure...Feiglinge... |LBR| Und wie nennen wir sie..?\n",
            " \ttext_classification_label: OTHER\n",
            "Tokenized: \n",
            " \ttokens: ['[UNK]', 'Frank', '[UNK]', 'Pas', '##emann', '[UNK]', 'Die', '[UNK]', 'Gru', '##ene', '##n', 'Wie', 'nennt', 'man', 'Männer', 'die', 'im', 'Krieg', 'ihr', 'Land', 'ihre', 'Lands', '##leute', 'ihre', 'Familien', 'ihre', 'Kamera', '##den', 'im', 'Stich', 'lassen', '.', '.', '[UNK]', '|', 'LB', '##R', '|', 'Des', '##erte', '##ure', '.', '.', '.', 'Fe', '##ig', '##linge', '.', '.', '.', '|', 'LB', '##R', '|', 'Und', 'wie', 'nennen', 'wir', 'sie', '.', '.', '[UNK]']\n",
            " \toffsets: [0, 5, 10, 15, 18, 16, 21, 24, 29, 32, 35, 29, 33, 39, 43, 50, 54, 57, 63, 67, 72, 77, 82, 88, 93, 102, 107, 113, 117, 120, 126, 132, 133, 134, 136, 137, 139, 140, 142, 145, 149, 152, 153, 154, 155, 157, 159, 164, 165, 166, 168, 169, 171, 172, 174, 178, 182, 189, 193, 196, 197, 198]\n",
            " \tstart_of_word: [True, False, False, False, False, True, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, True, True, True, True, True, False, False, False]\n",
            "Features: \n",
            " \tinput_ids: [3, 2, 992, 2, 11816, 7486, 2, 125, 2, 7671, 5119, 26898, 1316, 6698, 478, 3284, 30, 106, 2606, 254, 414, 682, 11514, 7586, 682, 2409, 682, 8967, 65, 106, 8170, 1641, 4813, 4813, 2, 24854, 12586, 26938, 24854, 2233, 11311, 2401, 4813, 4813, 4813, 5214, 80, 3623, 4813, 4813, 4813, 24854, 12586, 26938, 24854, 1356, 246, 8577, 232, 213, 4813, 4813, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "12/10/2019 15:12:24 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-472-0\n",
            "Clear Text: \n",
            " \ttext: @Rebellin70 Lindner hat es nur verdeutlicht was mittlerweile alles wissen....Die Grünen sind zu nichts zu gebrauchen...!\n",
            " \ttext_classification_label: OTHER\n",
            "Tokenized: \n",
            " \ttokens: ['[UNK]', 'Reb', '##elli', '##n', '##70', 'Lindner', 'hat', 'es', 'nur', 'verdeut', '##licht', 'was', 'mittlerweile', 'alles', 'wissen', '.', '.', '.', '.', 'Die', 'Grünen', 'sind', 'zu', 'nichts', 'zu', 'gebr', '##auch', '##en', '.', '.', '.', '[UNK]']\n",
            " \toffsets: [0, 5, 8, 12, 13, 12, 20, 24, 27, 31, 38, 44, 48, 61, 67, 73, 74, 75, 76, 77, 81, 88, 93, 96, 103, 106, 110, 114, 116, 117, 118, 119]\n",
            " \tstart_of_word: [True, False, False, False, False, True, True, True, True, True, False, True, True, True, True, False, False, False, False, False, True, True, True, True, True, True, False, False, False, False, False, False]\n",
            "Features: \n",
            " \tinput_ids: [3, 2, 23779, 11297, 26898, 4015, 26599, 193, 229, 356, 18689, 817, 961, 5330, 2368, 5029, 4813, 4813, 4813, 4813, 125, 5373, 287, 81, 2013, 81, 23963, 4174, 7, 4813, 4813, 4813, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "12/10/2019 15:12:24 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-2-0\n",
            "Clear Text: \n",
            " \ttext: @BROT_furdiewelt Ihr entscheidet auch nicht darüber. Es geht um eure Meinung: ist Israel für euch ein Apartheid-Staat: ja oder nein?\n",
            " \ttext_classification_label: OTHER\n",
            "Tokenized: \n",
            " \ttokens: ['[UNK]', 'BR', '##O', '##T', '[UNK]', 'fur', '##die', '##welt', 'Ihr', 'entscheidet', 'auch', 'nicht', 'darüber', '.', 'Es', 'geht', 'um', 'e', '##ure', 'Meinung', ':', 'ist', 'Israel', 'für', 'euch', 'ein', 'Ap', '##arth', '##eid', '-', 'Staat', ':', 'ja', 'oder', 'ne', '##in', '[UNK]']\n",
            " \toffsets: [0, 5, 7, 8, 9, 14, 17, 20, 17, 21, 33, 38, 44, 51, 53, 56, 61, 64, 65, 69, 76, 78, 82, 89, 93, 98, 102, 104, 108, 111, 112, 117, 119, 122, 127, 129, 131]\n",
            " \tstart_of_word: [True, False, False, False, False, False, False, False, True, True, True, True, True, False, True, True, True, True, False, True, False, True, True, True, True, True, True, False, False, False, False, False, True, True, True, False, False]\n",
            "Features: \n",
            " \tinput_ids: [3, 2, 11869, 26962, 26943, 2, 24058, 2930, 2190, 1673, 11762, 194, 149, 2683, 4813, 482, 1398, 259, 454, 2401, 7048, 5982, 127, 6908, 142, 21529, 39, 3948, 17719, 1281, 243, 2252, 5982, 3278, 309, 2055, 14, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "5010 Dicts [00:13, 363.61 Dicts/s]                      \n",
            "12/10/2019 15:12:35 - INFO - farm.data_handler.data_silo -   Loading dev set as a slice of train set\n",
            "12/10/2019 15:12:35 - INFO - farm.data_handler.data_silo -   Took 500 samples out of train set to create dev set (dev split is roughly 0.1)\n",
            "12/10/2019 15:12:35 - INFO - farm.data_handler.data_silo -   Loading test set from: ../data/germeval18/test.tsv\n",
            "12/10/2019 15:12:36 - INFO - farm.data_handler.data_silo -   Got ya 2 parallel workers to convert 3532 dictionaries to pytorch datasets (chunksize = 354)...\n",
            "12/10/2019 15:12:36 - INFO - farm.data_handler.data_silo -    0    0 \n",
            "12/10/2019 15:12:36 - INFO - farm.data_handler.data_silo -   /|\\  /w\\\n",
            "12/10/2019 15:12:36 - INFO - farm.data_handler.data_silo -   /'\\  / \\\n",
            "12/10/2019 15:12:36 - INFO - farm.data_handler.data_silo -     \n",
            "  0%|          | 0/3532 [00:00<?, ? Dicts/s]12/10/2019 15:12:37 - INFO - farm.data_handler.processor -   *** Show 3 random examples ***\n",
            "12/10/2019 15:12:37 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-338-0\n",
            "Clear Text: \n",
            " \ttext: @Lata_mariusz Klares Nein was die CDU anbetrifft, meine politische Heimat ist die SPD! Und mich wundert immer wenn einige Linken von der SPD mit einer Wagenknecht kein Problem hätten, was zum Beispiel die Flüchtlingspolitik anbetrifft, versucht die Linkspartei die AFD rechts zu überholen!\n",
            " \ttext_classification_label: OTHER\n",
            "Tokenized: \n",
            " \ttokens: ['[UNK]', 'Lat', '##a', '[UNK]', 'mar', '##ius', '##z', 'Klar', '##es', 'Nein', 'was', 'die', 'CDU', 'anb', '##et', '##rifft', ',', 'meine', 'politische', 'Heimat', 'ist', 'die', 'SPD', '[UNK]', 'Und', 'mich', 'wunder', '##t', 'immer', 'wenn', 'einige', 'Linken', 'von', 'der', 'SPD', 'mit', 'einer', 'Wagen', '##knecht', 'kein', 'Problem', 'hätten', ',', 'was', 'zum', 'Beispiel', 'die', 'Flüchtlings', '##politik', 'anb', '##et', '##rifft', ',', 'versucht', 'die', 'Linkspartei', 'die', 'AF', '##D', 'rechts', 'zu', 'über', '##holen', '[UNK]']\n",
            " \toffsets: [0, 5, 8, 9, 14, 17, 20, 14, 18, 21, 26, 30, 34, 38, 41, 43, 48, 50, 56, 67, 74, 78, 82, 85, 87, 91, 96, 102, 104, 110, 115, 122, 129, 133, 137, 141, 145, 151, 156, 163, 168, 176, 182, 184, 188, 192, 201, 205, 216, 224, 227, 229, 234, 236, 245, 249, 261, 265, 267, 269, 276, 279, 283, 288]\n",
            " \tstart_of_word: [True, False, False, False, False, False, False, True, False, True, True, True, True, True, False, False, False, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, False, True, False, False, False, True, True, True, True, True, False, True, True, True, False, False]\n",
            "Features: \n",
            " \tinput_ids: [3, 2, 16241, 26903, 2, 8905, 2096, 26916, 9621, 16, 12290, 961, 30, 4066, 10134, 75, 3695, 2036, 6667, 4386, 3060, 127, 30, 3014, 2, 1356, 3277, 13051, 26901, 922, 557, 1967, 15942, 88, 21, 3014, 114, 225, 5182, 23001, 1011, 2423, 2040, 2036, 961, 260, 2249, 30, 9246, 3663, 10134, 75, 3695, 2036, 4820, 30, 21471, 30, 22303, 26926, 3101, 81, 204, 7778, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "12/10/2019 15:12:37 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-350-0\n",
            "Clear Text: \n",
            " \ttext: Das Faszinierende an der Existenz des heterosexuellen Mannes ist seine stoische Bereitschaft, die Arschkarte des Lebens so gelassen wie möglich zu ertragen.\n",
            " \ttext_classification_label: OTHER\n",
            "Tokenized: \n",
            " \ttokens: ['Das', 'Fas', '##zin', '##ierende', 'an', 'der', 'Existenz', 'des', 'he', '##ter', '##osex', '##uellen', 'Mannes', 'ist', 'seine', 'sto', '##ische', 'Bereitschaft', ',', 'die', 'Ar', '##sch', '##karte', 'des', 'Lebens', 'so', 'gelassen', 'wie', 'möglich', 'zu', 'ert', '##ragen', '.']\n",
            " \toffsets: [0, 4, 7, 10, 18, 21, 25, 34, 38, 40, 43, 47, 54, 61, 65, 71, 74, 80, 92, 94, 98, 100, 103, 109, 113, 120, 123, 132, 136, 144, 147, 150, 155]\n",
            " \tstart_of_word: [True, True, False, False, True, True, True, True, True, False, False, False, True, True, True, True, False, True, False, True, True, False, False, True, True, True, True, True, True, True, True, False, False]\n",
            "Features: \n",
            " \tinput_ids: [3, 295, 18124, 4122, 4324, 104, 21, 7650, 91, 5726, 60, 12668, 3195, 10794, 127, 498, 17878, 262, 16673, 2036, 30, 317, 28, 11356, 91, 1427, 181, 10401, 246, 1465, 81, 21226, 4620, 4813, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "12/10/2019 15:12:37 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-64-0\n",
            "Clear Text: \n",
            " \ttext: @JoeHellBack @ThomasMichael71 @alexa_jung60 @Tschonka @dasstimmvieh @Mohrenpost Och, antifeministisch bin ich auch :)\n",
            " \ttext_classification_label: OTHER\n",
            "Tokenized: \n",
            " \ttokens: ['[UNK]', 'Joe', '##Hel', '##l', '##B', '##ack', '[UNK]', 'Thomas', '##Mich', '##ael', '##71', '[UNK]', 'al', '##ex', '##a', '[UNK]', 'jung', '##60', '[UNK]', 'Tsch', '##on', '##ka', '[UNK]', 'dass', '##tim', '##m', '##vi', '##eh', '[UNK]', 'Mo', '##hren', '##post', 'O', '##ch', ',', 'ant', '##if', '##emin', '##istisch', 'bin', 'ich', 'auch', ':', ')']\n",
            " \toffsets: [0, 5, 8, 11, 12, 13, 13, 18, 24, 28, 31, 30, 35, 37, 39, 40, 45, 49, 44, 49, 53, 55, 54, 59, 63, 66, 67, 69, 68, 73, 75, 79, 80, 81, 83, 85, 88, 90, 94, 102, 106, 110, 115, 116]\n",
            " \tstart_of_word: [True, False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, True, False, False, False, True, False, False, True, False, False, False, True, True, True, True, False]\n",
            "Features: \n",
            " \tinput_ids: [3, 2, 14047, 20243, 26907, 26925, 1672, 2, 3075, 19662, 2663, 6684, 2, 1119, 3244, 26903, 2, 8536, 3196, 2, 10368, 23, 2212, 2, 221, 21258, 26911, 10610, 1197, 2, 1094, 2306, 12750, 169, 8, 2036, 3631, 778, 14086, 4584, 4058, 1169, 194, 5982, 5133, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0]\n",
            "_____________________________________________________\n",
            "3540 Dicts [00:08, 418.10 Dicts/s]                      \n",
            "12/10/2019 15:12:44 - INFO - farm.data_handler.data_silo -   Examples in train: 4509\n",
            "12/10/2019 15:12:44 - INFO - farm.data_handler.data_silo -   Examples in dev  : 500\n",
            "12/10/2019 15:12:44 - INFO - farm.data_handler.data_silo -   Examples in test : 3532\n",
            "12/10/2019 15:12:44 - INFO - farm.data_handler.data_silo -   \n",
            "12/10/2019 15:12:44 - INFO - farm.data_handler.data_silo -   Max sequence length:     128\n",
            "12/10/2019 15:12:44 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 40.41428254601907\n",
            "12/10/2019 15:12:44 - INFO - farm.data_handler.data_silo -   Proportion clipped:      0.018629407850964737\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG8toIFclN2F",
        "colab_type": "text"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw4CJvz5lRbj",
        "colab_type": "text"
      },
      "source": [
        "In FARM, we make a strong distinction between the language model and prediction head so that you can mix and match different building blocks for your needs.\n",
        "\n",
        "For example, in the transfer learning paradigm, you might have the one language model that you will be using for both document classification and NER. Or you perhaps you have a pretrained language model which you would like to adapt to your domain, then use for a downstream task such as question answering. \n",
        "\n",
        "All this is possible within FARM and requires only the replacement of a few modular components, as we shall see below.\n",
        "\n",
        "Let's first have a look at how we might set up a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4TaqNaTlVmm",
        "colab_type": "code",
        "outputId": "7283f8de-09b5-46c6-8477-f50a1795df2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "# The language model is the foundation on which modern NLP systems are built.\n",
        "# They encapsulate a general understanding of sentence semantics\n",
        "# and are not specific to any one task.\n",
        "\n",
        "# Here we are using Google's BERT model as implemented by HuggingFace. \n",
        "# The model being loaded is a German model that we trained. \n",
        "# You can also change the MODEL_NAME_OR_PATH to point to a BERT model that you\n",
        "# have saved or download one connected to the HuggingFace repository.\n",
        "# See farm.modeling.language_model.PRETRAINED_MODEL_ARCHIVE_MAP for a list of\n",
        "# available models\n",
        "\n",
        "MODEL_NAME_OR_PATH = \"bert-base-german-cased\"\n",
        "\n",
        "language_model = LanguageModel.load(MODEL_NAME_OR_PATH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/10/2019 15:22:19 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpmrg4qz0m\n",
            "100%|██████████| 288/288 [00:00<00:00, 161859.78B/s]\n",
            "12/10/2019 15:22:20 - INFO - transformers.file_utils -   copying /tmp/tmpmrg4qz0m to cache at /root/.cache/torch/transformers/e653e2fe0970d519c5a3b6c0286e1630ad2f0eade78f82b4916ec945d6f06d48.958be95f8721c8cc3ff0998c94a7b77083ecd9345f62423db24ad2387d599c7d\n",
            "12/10/2019 15:22:20 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/e653e2fe0970d519c5a3b6c0286e1630ad2f0eade78f82b4916ec945d6f06d48.958be95f8721c8cc3ff0998c94a7b77083ecd9345f62423db24ad2387d599c7d\n",
            "12/10/2019 15:22:20 - INFO - transformers.file_utils -   removing temp file /tmp/tmpmrg4qz0m\n",
            "12/10/2019 15:22:20 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-config.json from cache at /root/.cache/torch/transformers/e653e2fe0970d519c5a3b6c0286e1630ad2f0eade78f82b4916ec945d6f06d48.958be95f8721c8cc3ff0998c94a7b77083ecd9345f62423db24ad2387d599c7d\n",
            "12/10/2019 15:22:20 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "12/10/2019 15:22:21 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpzcjnpz4n\n",
            "100%|██████████| 438869143/438869143 [00:38<00:00, 11343980.03B/s]\n",
            "12/10/2019 15:23:00 - INFO - transformers.file_utils -   copying /tmp/tmpzcjnpz4n to cache at /root/.cache/torch/transformers/e32f648561b03f77a129832928b7f16decdc5e0870f1e6558857e046169d4133.4e5eda3a0f09b32a0b7d1a9185034da1b3506d5c5b0c6880a7ca0122ab5eef2e\n",
            "12/10/2019 15:23:01 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/e32f648561b03f77a129832928b7f16decdc5e0870f1e6558857e046169d4133.4e5eda3a0f09b32a0b7d1a9185034da1b3506d5c5b0c6880a7ca0122ab5eef2e\n",
            "12/10/2019 15:23:01 - INFO - transformers.file_utils -   removing temp file /tmp/tmpzcjnpz4n\n",
            "12/10/2019 15:23:01 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-pytorch_model.bin from cache at /root/.cache/torch/transformers/e32f648561b03f77a129832928b7f16decdc5e0870f1e6558857e046169d4133.4e5eda3a0f09b32a0b7d1a9185034da1b3506d5c5b0c6880a7ca0122ab5eef2e\n",
            "12/10/2019 15:23:04 - INFO - farm.modeling.language_model -   Automatically detected language from language model name: german\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKpqm6solWce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A prediction head is a model that processes the output of the language model\n",
        "# for a specific task.\n",
        "# Prediction heads will look different depending on whether you're doing text classification\n",
        "# Named Entity Recognition (NER), question answering or some other task.\n",
        "# They should generate logits over the available prediction classes and contain methods\n",
        "# to convert these logits to losses or predictions \n",
        "\n",
        "# Here we use TextClassificationHead which receives a single fixed length sentence vector\n",
        "# and processes it using a feed forward neural network. layer_dims is a list of dimensions:\n",
        "# [input_dims, hidden_1_dims, hidden_2_dims ..., output_dims]\n",
        "\n",
        "# Here by default we have a single layer network.\n",
        "# It takes in a vector of length 768 (the default size of BERT's output).\n",
        "# It outputs a vector of length 2 (the number of classes in the GermEval18 (coarse) dataset)\n",
        "\n",
        "LAYER_DIMS = [768, 2]\n",
        "\n",
        "prediction_head = TextClassificationHead(layer_dims=LAYER_DIMS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQL1HGo2lZEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The language model and prediction head are coupled together in the Adaptive Model.\n",
        "# This class takes care of model saving and loading and also coordinates\n",
        "# cases where there is more than one prediction head.\n",
        "\n",
        "# EMBEDS_DROPOUT_PROB is the probability that an element of the output vector from the\n",
        "# language model will be set to zero.\n",
        "EMBEDS_DROPOUT_PROB = 0.1\n",
        "\n",
        "model = AdaptiveModel(\n",
        "    language_model=language_model,\n",
        "    prediction_heads=[prediction_head],\n",
        "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
        "    lm_output_types=[\"per_sequence\"],\n",
        "    device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE861jLalax1",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4db05tC7lcKy",
        "colab_type": "code",
        "outputId": "aad4c9c6-3a28-475c-a98e-30b983baa060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Here we initialize a Bert Adam optimizer that has a linear warmup and warmdown\n",
        "# Here you can set learning rate, the warmup proportion and number of epochs to train for\n",
        "\n",
        "LEARNING_RATE = 2e-5\n",
        "WARMUP_PROPORTION = 0.1\n",
        "N_EPOCHS = 1\n",
        "\n",
        "optimizer, warmup_linear = initialize_optimizer(\n",
        "    model=model,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_proportion=WARMUP_PROPORTION,\n",
        "    n_batches=len(data_silo.loaders[\"train\"]),\n",
        "    n_epochs=N_EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/10/2019 15:25:34 - INFO - farm.modeling.optimization -   Number of optimization steps: 141\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpkluUz3lfJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training loop handled by this\n",
        "# It will also trigger evaluation during training using the dev data\n",
        "# and after training using the test data.\n",
        "\n",
        "# Set N_GPU to a positive value if CUDA is available\n",
        "N_GPU = 1\n",
        "\n",
        "trainer = Trainer(\n",
        "    optimizer=optimizer,\n",
        "    data_silo=data_silo,\n",
        "    epochs=N_EPOCHS,\n",
        "    n_gpu=N_GPU,\n",
        "    warmup_linear=warmup_linear,\n",
        "    device=device,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuSTk5zAlc0A",
        "colab_type": "code",
        "outputId": "dbd1c23e-3f75-4733-8fea-4402b3fc68a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = trainer.train(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/10/2019 15:25:36 - INFO - farm.train -   \n",
            " \n",
            "\n",
            "          &&& &&  & &&             _____                   _             \n",
            "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
            "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
            "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
            "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
            "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
            " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
            "     &&     \\|||                                                   |___/\n",
            "             |||\n",
            "             |||\n",
            "             |||\n",
            "       , -=-~  .-^- _\n",
            "              `\n",
            "\n",
            "Train epoch 1/1:  71%|███████   | 100/141 [02:39<01:01,  1.51s/it]\n",
            "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 16/16 [00:07<00:00,  2.06it/s]\u001b[A12/10/2019 15:28:25 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 100 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "12/10/2019 15:28:25 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "12/10/2019 15:28:26 - INFO - farm.eval -   loss: 0.4148799114227295\n",
            "12/10/2019 15:28:26 - INFO - farm.eval -   task_name: text_classification\n",
            "12/10/2019 15:28:27 - INFO - farm.eval -   f1_macro: 0.7756489843215669\n",
            "12/10/2019 15:28:27 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       OTHER     0.8438    0.8359    0.8398       323\n",
            "     OFFENSE     0.7056    0.7175    0.7115       177\n",
            "\n",
            "    accuracy                         0.7940       500\n",
            "   macro avg     0.7747    0.7767    0.7756       500\n",
            "weighted avg     0.7948    0.7940    0.7944       500\n",
            "\n",
            "Train epoch 1/1: 100%|██████████| 141/141 [03:53<00:00,  1.44s/it]\n",
            "Evaluating: 100%|██████████| 111/111 [00:54<00:00,  2.02it/s]\n",
            "12/10/2019 15:30:25 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | TEST SET | AFTER 141 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "12/10/2019 15:30:25 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "12/10/2019 15:30:26 - INFO - farm.eval -   loss: 0.4651742023357854\n",
            "12/10/2019 15:30:26 - INFO - farm.eval -   task_name: text_classification\n",
            "12/10/2019 15:30:27 - INFO - farm.eval -   f1_macro: 0.7420496246639547\n",
            "12/10/2019 15:30:27 - INFO - farm.eval -   report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       OTHER     0.7906    0.9288    0.8542      2330\n",
            "     OFFENSE     0.7912    0.5233    0.6299      1202\n",
            "\n",
            "    accuracy                         0.7908      3532\n",
            "   macro avg     0.7909    0.7260    0.7420      3532\n",
            "weighted avg     0.7908    0.7908    0.7779      3532\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r8b3etug_F4",
        "colab_type": "code",
        "outputId": "e5c25b98-5f8c-4ce6-d6aa-4746346b6072",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Test your model on a sample (Inference)\n",
        "from farm.infer import Inferencer\n",
        "from pprint import PrettyPrinter\n",
        "\n",
        "infer_model = Inferencer(processor=processor, model=model, gpu=True)\n",
        "\n",
        "basic_texts = [\n",
        "    {\"text\": \"Martin ist ein Idiot\"},\n",
        "    {\"text\": \"Martin Müller spielt Handball in Berlin\"},\n",
        "]\n",
        "result = infer_model.inference_from_dicts(dicts=basic_texts)\n",
        "PrettyPrinter().pprint(result)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/10/2019 15:31:12 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "12/10/2019 15:31:12 - INFO - farm.infer -   Got ya 1 parallel workers to do inference on 2dicts (chunksize = 4)...\n",
            "12/10/2019 15:31:12 - INFO - farm.infer -    0 \n",
            "12/10/2019 15:31:12 - INFO - farm.infer -   /w\\\n",
            "12/10/2019 15:31:12 - INFO - farm.infer -   /'\\\n",
            "12/10/2019 15:31:12 - INFO - farm.infer -   \n",
            "  0%|          | 0/2 [00:00<?, ? Dicts/s]12/10/2019 15:31:12 - INFO - farm.data_handler.processor -   *** Show 3 random examples ***\n",
            "12/10/2019 15:31:12 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-0-0\n",
            "Clear Text: \n",
            " \ttext: Martin ist ein Idiot\n",
            "Tokenized: \n",
            " \ttokens: ['Martin', 'ist', 'ein', 'Id', '##iot']\n",
            " \toffsets: [0, 7, 11, 15, 17]\n",
            " \tstart_of_word: [True, True, True, True, False]\n",
            "Features: \n",
            " \tinput_ids: [3, 3810, 127, 39, 17034, 11643, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "12/10/2019 15:31:12 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-1-0\n",
            "Clear Text: \n",
            " \ttext: Martin Müller spielt Handball in Berlin\n",
            "Tokenized: \n",
            " \ttokens: ['Martin', 'Müller', 'spielt', 'Handball', 'in', 'Berlin']\n",
            " \toffsets: [0, 7, 14, 21, 30, 33]\n",
            " \tstart_of_word: [True, True, True, True, True, True]\n",
            "Features: \n",
            " \tinput_ids: [3, 3810, 5477, 3727, 11065, 50, 715, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "12/10/2019 15:31:12 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-0-0\n",
            "Clear Text: \n",
            " \ttext: Martin ist ein Idiot\n",
            "Tokenized: \n",
            " \ttokens: ['Martin', 'ist', 'ein', 'Id', '##iot']\n",
            " \toffsets: [0, 7, 11, 15, 17]\n",
            " \tstart_of_word: [True, True, True, True, False]\n",
            "Features: \n",
            " \tinput_ids: [3, 3810, 127, 39, 17034, 11643, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "\n",
            "Inferencing:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "4 Dicts [00:00, 27.24 Dicts/s]           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[{'predictions': [{'context': 'Martin ist ein Idiot',\n",
            "                   'end': None,\n",
            "                   'label': 'OFFENSE',\n",
            "                   'probability': 0.7827183,\n",
            "                   'start': None},\n",
            "                  {'context': 'Martin Müller spielt Handball in Berlin',\n",
            "                   'end': None,\n",
            "                   'label': 'OTHER',\n",
            "                   'probability': 0.87313026,\n",
            "                   'start': None}],\n",
            "  'task': 'text_classification'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0BAYL1Iliiv",
        "colab_type": "text"
      },
      "source": [
        "# Switch to NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYEnZo5cll3J",
        "colab_type": "text"
      },
      "source": [
        "In a transfer learning paradigm, there is a core computation that is shared amongst all tasks. FARM's modular structure means that you can easily swap out different building blocks to make the same language model work for many different tasks.\n",
        "\n",
        "We can adapt the above text classification model to NER by simply switching out the processor and prediction head."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prov6seQlmLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the new building blocks\n",
        "\n",
        "from farm.data_handler.processor import NERProcessor\n",
        "from farm.modeling.prediction_head import TokenClassificationHead\n",
        "ml_logger.init_experiment(experiment_name=\"Public_FARM\", run_name=\"Tutorial1_Colab_NER\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reITkXdilqL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This processor will preprocess the data for the CoNLL03 NER task\n",
        "ner_labels = [\"[PAD]\", \"X\", \"O\", \"B-MISC\", \"I-MISC\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-OTH\", \"I-OTH\"]\n",
        "\n",
        "ner_processor = NERProcessor(tokenizer=tokenizer, \n",
        "                             max_seq_len=128, \n",
        "                             data_dir=\"../data/conll03-de\",\n",
        "                             label_list=ner_labels,\n",
        "                             metric=\"seq_f1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ-ddMWvlswY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This prediction head is also a feed forward neural network but expects one\n",
        "# vector per token in the input sequence and will generate a set of logits\n",
        "# for each input\n",
        "\n",
        "LAYER_DIMS = [768, 13]\n",
        "\n",
        "ner_prediction_head = TokenClassificationHead(layer_dims=LAYER_DIMS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY0IYYBXlu00",
        "colab_type": "code",
        "outputId": "efe842aa-a0a8-4f2b-8c50-3a0b1aca98b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We can integrate these new pieces with the rest using this code\n",
        "# It is pretty much the same structure as what we had above for text classification\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EMBEDS_DROPOUT_PROB = 0.1\n",
        "LEARNING_RATE = 2e-5\n",
        "WARMUP_PROPORTION = 0.1\n",
        "N_EPOCHS = 1\n",
        "N_GPU = 1\n",
        "\n",
        "data_silo = DataSilo(\n",
        "    processor=ner_processor,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "model = AdaptiveModel(\n",
        "    language_model=language_model,\n",
        "    prediction_heads=[ner_prediction_head],\n",
        "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
        "    lm_output_types=[\"per_token\"],\n",
        "    device=device)\n",
        "\n",
        "optimizer, warmup_linear = initialize_optimizer(\n",
        "    model=model,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_proportion=WARMUP_PROPORTION,\n",
        "    n_batches=len(data_silo.loaders[\"train\"]),\n",
        "    n_epochs=N_EPOCHS)\n",
        "\n",
        "trainer = Trainer(\n",
        "    optimizer=optimizer,\n",
        "    data_silo=data_silo,\n",
        "    epochs=N_EPOCHS,\n",
        "    n_gpu=N_GPU,\n",
        "    warmup_linear=warmup_linear,\n",
        "    device=device,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/09/2019 15:38:43 - INFO - farm.data_handler.data_silo -   \n",
            "Loading data into the data silo ... \n",
            "              ______\n",
            "               |o  |   !\n",
            "   __          |:`_|---'-.\n",
            "  |__|______.-/ _ \\-----.|       \n",
            " (o)(o)------'\\ _ /     ( )      \n",
            " \n",
            "12/09/2019 15:38:43 - INFO - farm.data_handler.data_silo -   Loading train set from: ../data/conll03-de/train.txt \n",
            "12/09/2019 15:38:43 - INFO - farm.data_handler.utils -    Couldn't find ../data/conll03-de/train.txt locally. Trying to download ...\n",
            "12/09/2019 15:38:43 - INFO - farm.data_handler.utils -   downloading and extracting file conll03-de to dir /data\n",
            "12/09/2019 15:38:46 - INFO - farm.data_handler.data_silo -   Got ya 2 parallel workers to convert 24000 dictionaries to pytorch datasets (chunksize = 2000)...\n",
            "12/09/2019 15:38:46 - INFO - farm.data_handler.data_silo -    0    0 \n",
            "12/09/2019 15:38:46 - INFO - farm.data_handler.data_silo -   /w\\  /w\\\n",
            "12/09/2019 15:38:46 - INFO - farm.data_handler.data_silo -   / \\  /'\\\n",
            "12/09/2019 15:38:46 - INFO - farm.data_handler.data_silo -     \n",
            "  0%|          | 0/24000 [00:00<?, ? Dicts/s]12/09/2019 15:38:53 - INFO - farm.data_handler.processor -   *** Show 3 random examples ***\n",
            "12/09/2019 15:38:53 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-1808-0\n",
            "Clear Text: \n",
            " \ttext: Nach Angaben eines Polizeisprechers war George D. aus Ghana gegen 21.15 Uhr am Bahnhof Kaulsdorf Nord aus der U-Bahn gestiegen .\n",
            " \tner_label: ['O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O']\n",
            "Tokenized: \n",
            " \ttokens: ['Nach', 'Angaben', 'eines', 'Polizei', '##sprecher', '##s', 'war', 'George', 'D', '.', 'aus', 'Gh', '##ana', 'gegen', '21', '.', '15', 'Uhr', 'am', 'Bahnhof', 'Kau', '##ls', '##dorf', 'Nord', 'aus', 'der', 'U', '-', 'Bahn', 'gestiegen', '.']\n",
            " \toffsets: [0, 5, 13, 19, 26, 34, 36, 40, 47, 48, 50, 54, 56, 60, 66, 68, 69, 72, 76, 79, 87, 90, 92, 97, 102, 106, 110, 111, 112, 117, 127]\n",
            " \tstart_of_word: [True, True, True, True, False, False, True, True, True, False, True, True, False, True, True, False, False, True, True, True, True, False, False, True, True, True, True, False, False, True, True]\n",
            "Features: \n",
            " \tinput_ids: [3, 326, 2428, 443, 1908, 8397, 26902, 185, 5774, 36, 4813, 147, 13645, 2685, 383, 2439, 4813, 659, 1971, 235, 3774, 14937, 121, 2237, 1188, 147, 21, 137, 243, 1621, 10228, 4813, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tner_label_ids: [1, 2, 2, 2, 2, 1, 1, 2, 5, 6, 1, 2, 9, 1, 2, 2, 1, 1, 2, 2, 2, 9, 1, 1, 10, 2, 2, 2, 1, 1, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "12/09/2019 15:38:53 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-697-0\n",
            "Clear Text: \n",
            " \ttext: Von \" It's Time \" wurde zunächst \" Home \" ausgekoppelt , später auch \" Feeling Good \" und schließlich \" Save The Last Dance For Me \" .\n",
            " \tner_label: ['O', 'O', 'B-OTH', 'I-OTH', 'O', 'O', 'O', 'O', 'B-OTH', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OTH', 'I-OTH', 'O', 'O', 'O', 'O', 'B-OTH', 'I-OTH', 'I-OTH', 'I-OTH', 'I-OTH', 'I-OTH', 'O', 'O']\n",
            "Tokenized: \n",
            " \ttokens: ['Von', '\"', 'It', \"'\", 's', 'Time', '\"', 'wurde', 'zunächst', '\"', 'Home', '\"', 'ausge', '##kop', '##pel', '##t', ',', 'später', 'auch', '\"', 'Fe', '##el', '##ing', 'Good', '\"', 'und', 'schließlich', '\"', 'Sav', '##e', 'The', 'Last', 'Dance', 'For', 'Me', '\"', '.']\n",
            " \toffsets: [0, 4, 6, 8, 9, 11, 16, 18, 24, 33, 35, 40, 42, 47, 50, 53, 55, 57, 64, 69, 71, 73, 75, 79, 84, 86, 90, 102, 104, 107, 109, 113, 118, 124, 128, 131, 133]\n",
            " \tstart_of_word: [True, True, True, False, False, True, True, True, True, True, True, True, True, False, False, False, True, True, True, True, True, False, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True]\n",
            "Features: \n",
            " \tinput_ids: [3, 1073, 151, 2418, 7529, 19, 21165, 151, 192, 1359, 151, 21588, 151, 666, 8954, 3688, 26901, 2036, 878, 194, 151, 5214, 77, 270, 17924, 151, 42, 2232, 151, 18770, 26897, 1233, 8244, 21826, 1148, 1362, 151, 4813, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tner_label_ids: [1, 2, 2, 11, 1, 1, 12, 2, 2, 2, 2, 11, 2, 2, 1, 1, 1, 2, 2, 2, 2, 11, 1, 1, 12, 2, 2, 2, 2, 11, 1, 12, 12, 12, 12, 12, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "12/09/2019 15:38:53 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-290-0\n",
            "Clear Text: \n",
            " \ttext: Die ITM-Koordinaten für die Klagemauer in Jerusalem sind : :E 222286 m :N 631556 m Die Umrechnung vom UTM-Koordinatensystem in ITM-Koordinaten bzw. andersherum ist möglich .\n",
            " \tner_label: ['O', 'B-OTH', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OTH', 'O', 'B-OTH', 'O', 'O', 'O', 'O', 'O']\n",
            "Tokenized: \n",
            " \ttokens: ['Die', 'IT', '##M', '-', 'Koordinaten', 'für', 'die', 'Klage', '##mauer', 'in', 'Jerusalem', 'sind', ':', ':', 'E', '22', '##22', '##86', 'm', ':', 'N', '63', '##15', '##56', 'm', 'Die', 'Um', '##rechnung', 'vom', 'U', '##T', '##M', '-', 'Koordinaten', '##system', 'in', 'IT', '##M', '-', 'Koordinaten', 'bzw', '.', 'anders', '##her', '##um', 'ist', 'möglich', '.']\n",
            " \toffsets: [0, 4, 6, 7, 8, 20, 24, 28, 33, 39, 42, 52, 57, 59, 60, 62, 64, 66, 69, 71, 72, 74, 76, 78, 81, 83, 87, 89, 98, 102, 103, 104, 105, 106, 117, 124, 127, 129, 130, 131, 143, 146, 148, 154, 157, 160, 164, 172]\n",
            " \tstart_of_word: [True, True, False, False, False, True, True, True, False, True, True, True, True, True, False, True, False, False, True, True, False, True, False, False, True, True, True, False, True, True, False, False, False, False, False, True, True, False, False, False, True, False, True, False, False, True, True, True]\n",
            "Features: \n",
            " \tinput_ids: [3, 125, 11220, 26929, 243, 26433, 142, 30, 1590, 8823, 50, 12767, 287, 5982, 5982, 55, 1389, 5484, 8847, 59, 5982, 102, 10477, 3063, 9248, 59, 125, 491, 5293, 275, 137, 26943, 26929, 243, 26433, 3059, 50, 11220, 26929, 243, 26433, 1372, 4813, 3602, 320, 107, 127, 1465, 4813, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tner_label_ids: [1, 2, 11, 1, 1, 1, 2, 2, 2, 1, 2, 9, 2, 2, 2, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 11, 1, 1, 1, 1, 1, 2, 11, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "100%|██████████| 24000/24000 [00:47<00:00, 509.31 Dicts/s]\n",
            "12/09/2019 15:39:33 - INFO - farm.data_handler.data_silo -   Loading dev set from: ../data/conll03-de/dev.txt\n",
            "12/09/2019 15:39:33 - INFO - farm.data_handler.data_silo -   Got ya 2 parallel workers to convert 2200 dictionaries to pytorch datasets (chunksize = 220)...\n",
            "12/09/2019 15:39:33 - INFO - farm.data_handler.data_silo -    0    0 \n",
            "12/09/2019 15:39:33 - INFO - farm.data_handler.data_silo -   /w\\  /w\\\n",
            "12/09/2019 15:39:33 - INFO - farm.data_handler.data_silo -   /'\\  / \\\n",
            "12/09/2019 15:39:33 - INFO - farm.data_handler.data_silo -     \n",
            "  0%|          | 0/2200 [00:00<?, ? Dicts/s]12/09/2019 15:39:34 - INFO - farm.data_handler.processor -   *** Show 3 random examples ***\n",
            "12/09/2019 15:39:34 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-91-0\n",
            "Clear Text: \n",
            " \ttext: Auch wenn das Ende der Köhlertradition abzusehen ist , so hatten die Köhlereien doch großen Anteil an der Geschichte des Nassachtals .\n",
            " \tner_label: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O']\n",
            "Tokenized: \n",
            " \ttokens: ['Auch', 'wenn', 'das', 'Ende', 'der', 'Köhler', '##tra', '##dition', 'abzu', '##sehen', 'ist', ',', 'so', 'hatten', 'die', 'Köhler', '##eien', 'doch', 'großen', 'Anteil', 'an', 'der', 'Geschichte', 'des', 'Nas', '##sa', '##cht', '##als', '.']\n",
            " \toffsets: [0, 5, 10, 14, 19, 23, 29, 32, 39, 43, 49, 53, 55, 58, 65, 69, 75, 80, 85, 92, 99, 102, 106, 117, 121, 124, 126, 129, 133]\n",
            " \tstart_of_word: [True, True, True, True, True, True, False, False, True, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, False, False, True]\n",
            "Features: \n",
            " \tinput_ids: [3, 831, 557, 93, 926, 21, 17740, 277, 15175, 2935, 2115, 127, 2036, 181, 1520, 30, 17740, 3575, 1575, 1714, 4566, 104, 21, 2009, 91, 19017, 4400, 191, 837, 4813, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tner_label_ids: [1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 9, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "12/09/2019 15:39:34 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-190-0\n",
            "Clear Text: \n",
            " \ttext: Derweil freut sich Nico G. auf seinen nächsten Einsatz .\n",
            " \tner_label: ['O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O']\n",
            "Tokenized: \n",
            " \ttokens: ['Der', '##weil', 'freut', 'sich', 'Nic', '##o', 'G', '.', 'auf', 'seinen', 'nächsten', 'Einsatz', '.']\n",
            " \toffsets: [0, 3, 8, 14, 19, 22, 24, 25, 27, 31, 38, 47, 55]\n",
            " \tstart_of_word: [True, False, True, True, True, False, True, False, True, True, True, True, True]\n",
            "Features: \n",
            " \tinput_ids: [3, 233, 11491, 20371, 144, 7651, 26910, 61, 4813, 115, 800, 3380, 2200, 4813, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tner_label_ids: [1, 2, 1, 2, 2, 5, 1, 6, 1, 2, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "12/09/2019 15:39:34 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-135-0\n",
            "Clear Text: \n",
            " \ttext: Bei der Reflexionsseismik , die zum Einsatz komme , erklärt Andreas Leisdon , würden zwei Lkws hintereinander fahren und in Abständen Vibrationsplatten auf den Boden setzen .\n",
            " \tner_label: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Tokenized: \n",
            " \ttokens: ['Bei', 'der', 'Reflex', '##ions', '##se', '##ism', '##ik', ',', 'die', 'zum', 'Einsatz', 'komme', ',', 'erklärt', 'Andreas', 'Lei', '##s', '##don', ',', 'würden', 'zwei', 'Lkw', '##s', 'hintereinander', 'fahren', 'und', 'in', 'Abständen', 'Vi', '##bra', '##ti', '##ons', '##platten', 'auf', 'den', 'Boden', 'setzen', '.']\n",
            " \toffsets: [0, 4, 8, 14, 18, 20, 23, 26, 28, 32, 36, 44, 50, 52, 60, 68, 71, 72, 76, 78, 85, 90, 93, 95, 110, 117, 121, 124, 134, 136, 139, 141, 144, 152, 156, 160, 166, 173]\n",
            " \tstart_of_word: [True, True, True, False, False, False, False, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, False, True, True, True, True, True, True, False, False, False, False, True, True, True, True, True]\n",
            "Features: \n",
            " \tinput_ids: [3, 467, 21, 20612, 1403, 145, 26507, 172, 2036, 30, 260, 2200, 8072, 2036, 3058, 5891, 1793, 26902, 11840, 2036, 2488, 382, 18994, 26902, 25251, 7452, 42, 50, 25548, 14615, 6572, 15099, 1817, 10327, 115, 86, 3055, 6047, 4813, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tner_label_ids: [1, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 5, 6, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "100%|██████████| 2200/2200 [00:04<00:00, 492.88 Dicts/s]\n",
            "12/09/2019 15:39:38 - INFO - farm.data_handler.data_silo -   Loading test set from: ../data/conll03-de/test.txt\n",
            "12/09/2019 15:39:38 - INFO - farm.data_handler.data_silo -   Got ya 2 parallel workers to convert 5100 dictionaries to pytorch datasets (chunksize = 510)...\n",
            "12/09/2019 15:39:38 - INFO - farm.data_handler.data_silo -    0    0 \n",
            "12/09/2019 15:39:38 - INFO - farm.data_handler.data_silo -   /w\\  /w\\\n",
            "12/09/2019 15:39:38 - INFO - farm.data_handler.data_silo -   / \\  /'\\\n",
            "12/09/2019 15:39:38 - INFO - farm.data_handler.data_silo -     \n",
            "  0%|          | 0/5100 [00:00<?, ? Dicts/s]12/09/2019 15:39:40 - INFO - farm.data_handler.processor -   *** Show 3 random examples ***\n",
            "12/09/2019 15:39:40 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-406-0\n",
            "Clear Text: \n",
            " \ttext: Auf lange Sicht halten die Patienten ihre Diät dann nicht durch .\n",
            " \tner_label: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Tokenized: \n",
            " \ttokens: ['Auf', 'lange', 'Sicht', 'halten', 'die', 'Patienten', 'ihre', 'Di', '##ät', 'dann', 'nicht', 'durch', '.']\n",
            " \toffsets: [0, 4, 10, 16, 23, 27, 37, 42, 44, 47, 52, 58, 64]\n",
            " \tstart_of_word: [True, True, True, True, True, True, True, True, False, True, True, True, True]\n",
            "Features: \n",
            " \tinput_ids: [3, 315, 2197, 3799, 3721, 30, 5501, 682, 1824, 220, 670, 149, 261, 4813, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tner_label_ids: [1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "12/09/2019 15:39:40 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-494-0\n",
            "Clear Text: \n",
            " \ttext: Services Obama plant Abgabe für gerettete Banken WASHINGTON : US-Präsident Barack Obama erwägt die Einführung einer Abgabe , mit der der vom Staat gerettete Finanzsektor einen Beitrag zum Ausgleich des hohen Haushaltsdefizits leisten soll .\n",
            " \tner_label: ['O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Tokenized: \n",
            " \ttokens: ['Services', 'Obama', 'plant', 'Abgabe', 'für', 'gerettet', '##e', 'Banken', 'W', '##AS', '##H', '##IN', '##G', '##TO', '##N', ':', 'US', '-', 'Präsident', 'Barack', 'Obama', 'erwä', '##gt', 'die', 'Einführung', 'einer', 'Abgabe', ',', 'mit', 'der', 'der', 'vom', 'Staat', 'gerettet', '##e', 'Finanz', '##sektor', 'einen', 'Beitrag', 'zum', 'Ausgleich', 'des', 'hohen', 'Haushalts', '##de', '##fizit', '##s', 'leisten', 'soll', '.']\n",
            " \toffsets: [0, 9, 15, 21, 28, 32, 40, 42, 49, 50, 52, 53, 55, 56, 58, 60, 62, 64, 65, 75, 82, 88, 92, 95, 99, 110, 116, 123, 125, 129, 133, 137, 141, 147, 155, 157, 163, 170, 176, 184, 188, 198, 202, 208, 217, 219, 224, 226, 234, 239]\n",
            " \tstart_of_word: [True, True, True, True, True, True, False, True, True, False, False, False, False, False, False, True, True, False, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, False, False, False, True, True, True]\n",
            "Features: \n",
            " \tinput_ids: [3, 26361, 9632, 15032, 10836, 142, 16784, 26897, 5167, 79, 12312, 26941, 9395, 26930, 12422, 26947, 5982, 960, 243, 2931, 17950, 9632, 3477, 202, 30, 6165, 225, 10836, 2036, 114, 21, 21, 275, 2252, 16784, 26897, 1747, 20128, 303, 5689, 260, 6106, 91, 3661, 6444, 57, 15612, 26902, 7180, 459, 4813, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tner_label_ids: [1, 2, 5, 2, 2, 2, 2, 1, 2, 9, 1, 1, 1, 1, 1, 1, 2, 9, 1, 1, 5, 6, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "12/09/2019 15:39:40 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-25-0\n",
            "Clear Text: \n",
            " \ttext: Der wichtigste Industriezweig ist die Nahrungsmittel- und Getränkeindustrie , gefolgt von der Metall- und der Textilindustrie .\n",
            " \tner_label: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Tokenized: \n",
            " \ttokens: ['Der', 'wichtigste', 'Industrie', '##zweig', 'ist', 'die', 'Nahrungsmittel', '-', 'und', 'Getränke', '##industrie', ',', 'gefolgt', 'von', 'der', 'Metall', '-', 'und', 'der', 'Textil', '##industrie', '.']\n",
            " \toffsets: [0, 4, 15, 24, 30, 34, 38, 52, 54, 58, 66, 76, 78, 86, 90, 94, 100, 102, 106, 110, 116, 126]\n",
            " \tstart_of_word: [True, True, True, False, True, True, True, False, True, True, False, True, True, True, True, True, False, True, True, True, False, True]\n",
            "Features: \n",
            " \tinput_ids: [3, 233, 10704, 4121, 17512, 127, 30, 25226, 243, 42, 21076, 6065, 2036, 11554, 88, 21, 6985, 243, 42, 21, 15838, 6065, 4813, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tinitial_mask: [0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tner_label_ids: [1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "100%|██████████| 5100/5100 [00:09<00:00, 519.64 Dicts/s]\n",
            "12/09/2019 15:39:48 - INFO - farm.data_handler.data_silo -   Examples in train: 24000\n",
            "12/09/2019 15:39:48 - INFO - farm.data_handler.data_silo -   Examples in dev  : 2200\n",
            "12/09/2019 15:39:48 - INFO - farm.data_handler.data_silo -   Examples in test : 5100\n",
            "12/09/2019 15:39:48 - INFO - farm.data_handler.data_silo -   \n",
            "12/09/2019 15:39:48 - INFO - farm.data_handler.data_silo -   Max sequence length:     83\n",
            "12/09/2019 15:39:48 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 26.766625\n",
            "12/09/2019 15:39:48 - INFO - farm.data_handler.data_silo -   Proportion clipped:      0.0\n",
            "12/09/2019 15:39:49 - INFO - farm.modeling.optimization -   Number of optimization steps: 750\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rbTzCDslxH0",
        "colab_type": "code",
        "outputId": "965fdf51-5502-4dd4-bcf5-b284049268a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = trainer.train(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/09/2019 15:39:50 - INFO - farm.train -   \n",
            " \n",
            "\n",
            "          &&& &&  & &&             _____                   _             \n",
            "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
            "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
            "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
            "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
            "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
            " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
            "     &&     \\|||                                                   |___/\n",
            "             |||\n",
            "             |||\n",
            "             |||\n",
            "       , -=-~  .-^- _\n",
            "              `\n",
            "\n",
            "Train epoch 1/1:  13%|█▎        | 100/750 [01:40<10:22,  1.04it/s]\n",
            "Evaluating:   0%|          | 0/69 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  43%|████▎     | 30/69 [00:10<00:13,  2.91it/s]\u001b[A\n",
            "Evaluating:  87%|████████▋ | 60/69 [00:20<00:03,  2.91it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 69/69 [00:23<00:00,  2.92it/s]\u001b[A12/09/2019 15:41:56 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 100 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "12/09/2019 15:41:56 - INFO - farm.eval -   \n",
            " _________ ner _________\n",
            "12/09/2019 15:41:56 - INFO - farm.eval -   loss: 2.6210123122822155\n",
            "12/09/2019 15:41:56 - INFO - farm.eval -   task_name: ner\n",
            "12/09/2019 15:41:57 - INFO - farm.eval -   seq_f1: 0.7135752440596795\n",
            "12/09/2019 15:41:57 - INFO - farm.eval -   report: \n",
            "            precision    recall  f1-score   support\n",
            "\n",
            "      PER       0.89      0.88      0.88       731\n",
            "      ORG       0.61      0.61      0.61       590\n",
            "      LOC       0.83      0.82      0.83      1050\n",
            "      OTH       0.30      0.24      0.27       303\n",
            "\n",
            "micro avg       0.70      0.72      0.71      2674\n",
            "macro avg       0.74      0.72      0.73      2674\n",
            "\n",
            "Train epoch 1/1:  27%|██▋       | 200/750 [03:44<08:40,  1.06it/s]\n",
            "Evaluating:   0%|          | 0/69 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  42%|████▏     | 29/69 [00:10<00:13,  2.90it/s]\u001b[A\n",
            "Evaluating:  84%|████████▍ | 58/69 [00:20<00:03,  2.90it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 69/69 [00:23<00:00,  2.91it/s]\u001b[A12/09/2019 15:44:01 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 200 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "12/09/2019 15:44:01 - INFO - farm.eval -   \n",
            " _________ ner _________\n",
            "12/09/2019 15:44:01 - INFO - farm.eval -   loss: 1.857765285318548\n",
            "12/09/2019 15:44:01 - INFO - farm.eval -   task_name: ner\n",
            "12/09/2019 15:44:02 - INFO - farm.eval -   seq_f1: 0.7800461565773122\n",
            "12/09/2019 15:44:02 - INFO - farm.eval -   report: \n",
            "            precision    recall  f1-score   support\n",
            "\n",
            "      PER       0.86      0.92      0.89       731\n",
            "      ORG       0.66      0.78      0.72       590\n",
            "      LOC       0.85      0.85      0.85      1050\n",
            "      OTH       0.53      0.56      0.54       303\n",
            "\n",
            "micro avg       0.74      0.82      0.78      2674\n",
            "macro avg       0.78      0.82      0.80      2674\n",
            "\n",
            "Train epoch 1/1:  40%|████      | 300/750 [05:49<07:04,  1.06it/s]\n",
            "Evaluating:   0%|          | 0/69 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  42%|████▏     | 29/69 [00:10<00:13,  2.89it/s]\u001b[A\n",
            "Evaluating:  84%|████████▍ | 58/69 [00:20<00:03,  2.89it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 69/69 [00:23<00:00,  2.90it/s]\u001b[A12/09/2019 15:46:05 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 300 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "12/09/2019 15:46:05 - INFO - farm.eval -   \n",
            " _________ ner _________\n",
            "12/09/2019 15:46:06 - INFO - farm.eval -   loss: 1.686125303181735\n",
            "12/09/2019 15:46:06 - INFO - farm.eval -   task_name: ner\n",
            "12/09/2019 15:46:06 - INFO - farm.eval -   seq_f1: 0.8188379646337063\n",
            "12/09/2019 15:46:06 - INFO - farm.eval -   report: \n",
            "            precision    recall  f1-score   support\n",
            "\n",
            "      PER       0.91      0.90      0.91       731\n",
            "      ORG       0.71      0.78      0.74       590\n",
            "      LOC       0.85      0.92      0.88      1050\n",
            "      OTH       0.64      0.62      0.63       303\n",
            "\n",
            "micro avg       0.79      0.85      0.82      2674\n",
            "macro avg       0.81      0.85      0.83      2674\n",
            "\n",
            "Train epoch 1/1:  53%|█████▎    | 400/750 [07:54<05:29,  1.06it/s]\n",
            "Evaluating:   0%|          | 0/69 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  42%|████▏     | 29/69 [00:10<00:13,  2.90it/s]\u001b[A\n",
            "Evaluating:  86%|████████▌ | 59/69 [00:20<00:03,  2.90it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 69/69 [00:23<00:00,  2.91it/s]\u001b[A12/09/2019 15:48:10 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 400 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "12/09/2019 15:48:10 - INFO - farm.eval -   \n",
            " _________ ner _________\n",
            "12/09/2019 15:48:11 - INFO - farm.eval -   loss: 1.5840216224843806\n",
            "12/09/2019 15:48:11 - INFO - farm.eval -   task_name: ner\n",
            "12/09/2019 15:48:11 - INFO - farm.eval -   seq_f1: 0.8256549232158988\n",
            "12/09/2019 15:48:11 - INFO - farm.eval -   report: \n",
            "            precision    recall  f1-score   support\n",
            "\n",
            "      PER       0.93      0.90      0.91       731\n",
            "      ORG       0.75      0.81      0.78       590\n",
            "      LOC       0.88      0.90      0.89      1050\n",
            "      OTH       0.57      0.69      0.62       303\n",
            "\n",
            "micro avg       0.80      0.85      0.83      2674\n",
            "macro avg       0.83      0.85      0.84      2674\n",
            "\n",
            "Train epoch 1/1:  67%|██████▋   | 500/750 [09:59<03:57,  1.05it/s]\n",
            "Evaluating:   0%|          | 0/69 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  42%|████▏     | 29/69 [00:10<00:13,  2.87it/s]\u001b[A\n",
            "Evaluating:  84%|████████▍ | 58/69 [00:20<00:03,  2.87it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 69/69 [00:24<00:00,  2.87it/s]\u001b[A12/09/2019 15:50:15 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 500 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "12/09/2019 15:50:15 - INFO - farm.eval -   \n",
            " _________ ner _________\n",
            "12/09/2019 15:50:16 - INFO - farm.eval -   loss: 1.4582151924480091\n",
            "12/09/2019 15:50:16 - INFO - farm.eval -   task_name: ner\n",
            "12/09/2019 15:50:16 - INFO - farm.eval -   seq_f1: 0.8426782798040283\n",
            "12/09/2019 15:50:16 - INFO - farm.eval -   report: \n",
            "            precision    recall  f1-score   support\n",
            "\n",
            "      PER       0.92      0.91      0.92       731\n",
            "      ORG       0.75      0.84      0.79       590\n",
            "      LOC       0.89      0.92      0.90      1050\n",
            "      OTH       0.66      0.64      0.65       303\n",
            "\n",
            "micro avg       0.82      0.87      0.84      2674\n",
            "macro avg       0.84      0.87      0.85      2674\n",
            "\n",
            "Train epoch 1/1:  80%|████████  | 600/750 [12:04<02:21,  1.06it/s]\n",
            "Evaluating:   0%|          | 0/69 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  42%|████▏     | 29/69 [00:10<00:13,  2.88it/s]\u001b[A\n",
            "Evaluating:  84%|████████▍ | 58/69 [00:20<00:03,  2.89it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 69/69 [00:23<00:00,  2.90it/s]\u001b[A12/09/2019 15:52:20 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 600 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "12/09/2019 15:52:20 - INFO - farm.eval -   \n",
            " _________ ner _________\n",
            "12/09/2019 15:52:20 - INFO - farm.eval -   loss: 1.4244600170308894\n",
            "12/09/2019 15:52:20 - INFO - farm.eval -   task_name: ner\n",
            "12/09/2019 15:52:21 - INFO - farm.eval -   seq_f1: 0.8415877640203934\n",
            "12/09/2019 15:52:21 - INFO - farm.eval -   report: \n",
            "            precision    recall  f1-score   support\n",
            "\n",
            "      PER       0.92      0.92      0.92       731\n",
            "      ORG       0.74      0.83      0.78       590\n",
            "      LOC       0.90      0.92      0.91      1050\n",
            "      OTH       0.70      0.60      0.65       303\n",
            "\n",
            "micro avg       0.82      0.86      0.84      2674\n",
            "macro avg       0.85      0.86      0.85      2674\n",
            "\n",
            "Train epoch 1/1:  93%|█████████▎| 700/750 [14:08<00:47,  1.06it/s]\n",
            "Evaluating:   0%|          | 0/69 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  42%|████▏     | 29/69 [00:10<00:13,  2.90it/s]\u001b[A\n",
            "Evaluating:  84%|████████▍ | 58/69 [00:20<00:03,  2.89it/s]\u001b[A\n",
            "Evaluating: 100%|██████████| 69/69 [00:23<00:00,  2.90it/s]\u001b[A12/09/2019 15:54:25 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 700 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "12/09/2019 15:54:25 - INFO - farm.eval -   \n",
            " _________ ner _________\n",
            "12/09/2019 15:54:25 - INFO - farm.eval -   loss: 1.4030878114700318\n",
            "12/09/2019 15:54:25 - INFO - farm.eval -   task_name: ner\n",
            "12/09/2019 15:54:26 - INFO - farm.eval -   seq_f1: 0.8444767441860465\n",
            "12/09/2019 15:54:26 - INFO - farm.eval -   report: \n",
            "            precision    recall  f1-score   support\n",
            "\n",
            "      PER       0.92      0.91      0.92       731\n",
            "      ORG       0.75      0.82      0.79       590\n",
            "      LOC       0.89      0.92      0.90      1050\n",
            "      OTH       0.70      0.67      0.68       303\n",
            "\n",
            "micro avg       0.82      0.87      0.84      2674\n",
            "macro avg       0.84      0.87      0.86      2674\n",
            "\n",
            "Train epoch 1/1: 100%|██████████| 750/750 [15:24<00:00,  1.06it/s]\n",
            "Evaluating: 100%|██████████| 160/160 [00:55<00:00,  2.90it/s]\n",
            "12/09/2019 15:56:11 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | TEST SET | AFTER 750 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "12/09/2019 15:56:11 - INFO - farm.eval -   \n",
            " _________ ner _________\n",
            "12/09/2019 15:56:11 - INFO - farm.eval -   loss: 1.4082207532022513\n",
            "12/09/2019 15:56:11 - INFO - farm.eval -   task_name: ner\n",
            "12/09/2019 15:56:12 - INFO - farm.eval -   seq_f1: 0.8387303622956076\n",
            "12/09/2019 15:56:12 - INFO - farm.eval -   report: \n",
            "            precision    recall  f1-score   support\n",
            "\n",
            "      PER       0.92      0.91      0.92      1693\n",
            "      OTH       0.65      0.61      0.63       778\n",
            "      LOC       0.90      0.91      0.90      2376\n",
            "      ORG       0.80      0.78      0.79      1330\n",
            "\n",
            "micro avg       0.83      0.85      0.84      6177\n",
            "macro avg       0.85      0.85      0.85      6177\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOOLFVjdasOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}